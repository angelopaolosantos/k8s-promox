- hosts: controlplane-1
  become: true
  tasks:

  - name: Check if cluster is initialized
    shell: kubectl get cs | grep -c ok
    register: kubeadm_status
    become_user: ubuntu
    failed_when: kubeadm_status.rc == 2 # grep returns error code 2 for exceptions
    changed_when: false # so that the task won't be reported as "changed" every single time

  - name: Reset Cluster if initialized but not configured in kubectl
    shell: |
      kubeadm reset -f # force reset misconfigured cluster
      rm -rfv /etc/kubernetes /var/lib/kubelet /var/lib/etcd /etc/cni/net.d
      iptables --flush
    when: kubeadm_status.stdout == "0"
    failed_when: false
    changed_when: false

  - name: Generate certificate key
    shell: |
      kubeadm certs certificate-key > /var/tmp/init-config-cert-key
    args:
      creates: /var/tmp/init-config-cert-key

  - name: Generate certificate key
    fetch: 
      src: /var/tmp/init-config-cert-key
      dest: fetch/{{ inventory_hostname }}/init-cluster/init-config-cert-key
      flat: yes

  - name: register cert key
    slurp:
      src: /var/tmp/init-config-cert-key
    register: init_config_cert_key

  - name: Print returned information
    ansible.builtin.debug:
      msg: "{{ init_config_cert_key.content | b64decode }}"

  - name: Set cert key
    set_fact:
      cert_key: "{{ init_config_cert_key.content | b64decode }}"
  
  - name: Copy cluster config into /opt/cluster-config.yaml
    copy:
      dest: "/opt/cluster-config.yaml"
      content: |
        apiVersion: kubeadm.k8s.io/v1beta3
        kind: InitConfiguration
        localAPIEndpoint:
          advertiseAddress: 192.168.254.101
          bindPort: 6443
        certificateKey: {{ init_config_cert_key.content | b64decode }}
        ---
        apiVersion: kubeadm.k8s.io/v1beta3
        kind: ClusterConfiguration
        networking:
          serviceSubnet: "10.100.0.0/16"
          podSubnet: "10.244.0.0/16"
        # kubernetesVersion: stable
        controlPlaneEndpoint: "192.168.254.110:6443" # Adjust as needed, usually an external load balancer
        apiServer:
          extraArgs:
            oidc-issuer-url: "https://keycloak.deviantlab.duckdns.org/auth/realms/k8s-realm"
            oidc-client-id: "k8s-client"
        #     oidc-ca-file: "/etc/kubernetes/pki/ca.crt"
            oidc-username-prefix: "oidc:"
            oidc-groups-prefix: "oidc:"
            oidc-username-claim: "preferred_username"
            oidc-groups-claim: "groups"
        # controllerManager:
        #   extraArgs:
        #     oidc-issuer-url: "https://example.com/issuer"
        #     oidc-client-id: "your-client-id"
        #     oidc-ca-file: "/etc/kubernetes/pki/oidc-ca.crt"
        # scheduler:
        #   extraArgs:
        #     oidc-issuer-url: "https://example.com/issuer"
        #     oidc-client-id: "your-client-id"
        #     oidc-ca-file: "/etc/kubernetes/pki/oidc-ca.crt"
        ---
        kind: KubeletConfiguration
        apiVersion: kubelet.config.k8s.io/v1beta1
        cgroupDriver: systemd

  - name: Initialize Cluster
    shell: kubeadm init --config /opt/cluster-config.yaml --upload-certs
    when: kubeadm_status.stdout == "0"

  - debug: var=kubeadm_status.stdout_lines

  - name: Create .kube Directory
    file:
      path: "~/.kube"
      state: directory
    become_user: ubuntu

  - name: Copy /etc/kubernetes/admin.conf to local folder
    become: true
    copy:
      src: /etc/kubernetes/admin.conf
      remote_src: yes
      dest: /home/ubuntu/.kube/config

  - name: Download admin.conf
    ansible.builtin.fetch:
      src: /etc/kubernetes/admin.conf
      dest: fetch/{{ inventory_hostname }}/.kube/
      flat: yes

  - name: Set .kube Directory Owner and Group to vagrant
    shell:
      cmd: sudo chown $(id -u):$(id -g) $HOME/.kube/config
    become_user: ubuntu

  - name: Kubernetes configuration file should not be group-readable.
    shell:
      cmd: sudo chmod go-r ~/.kube/config
    become_user: ubuntu

  - name: Get join command
    shell: kubeadm token create --print-join-command 2>/dev/null
    register: join_command_raw

  - name: Set join command
    set_fact:
      join_command: "{{ join_command_raw.stdout_lines[0] }}"

  - name: Install CNI - Calico
    shell:
      cmd: |
        kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/calico.yaml
    ignore_errors: true
    become_user: ubuntu

  - name: Install helm if not exists
    unarchive:
      src: https://get.helm.sh/helm-v3.14.0-linux-amd64.tar.gz
      dest: /usr/local/bin
      extra_opts: "--strip-components=1"
      owner: root
      group: root
      mode: 0755
      remote_src: true
    args:
      creates: /usr/local/bin/helm

- hosts: controlplane-2, controlplane-3
  become: true
  tasks:
  - name: join cluster
    shell: |
      echo "{{ hostvars['192.168.254.101'].join_command }} --control-plane --certificate-key {{ hostvars['192.168.254.101'].cert_key }}" >> /opt/node_joined.txt
      {{ hostvars['192.168.254.101'].join_command }} --control-plane --certificate-key {{ hostvars['192.168.254.101'].cert_key }}
    args:
      creates: /opt/node_joined.txt # Runs task only once since file is created once

- hosts: workers
  become: true
  tasks:
  - name: join cluster
    shell: "{{ hostvars['192.168.254.101'].join_command }} >> /opt/node_joined.txt"
    args:
      creates: /opt/node_joined.txt # Runs task only once since file is created once
  
- name: Fix Nodes NotReady Status
  hosts: cluster
  become: true
  tasks:
    - name: systemctl daemon-reload
      ansible.builtin.systemd:
        daemon_reload: yes
    - name: Restart kubelet service
      ansible.builtin.systemd:
        name: kubelet
        state: restarted
    - name: Restart containerd service
      ansible.builtin.systemd:
        name: containerd
        state: restarted
  tags:
    - restart_services